{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing (NLP) Repository\n",
    "\n",
    "This repository serves as a comprehensive guide to Natural Language Processing (NLP), covering various concepts, techniques, and implementations. Each section includes theoretical explanations and practical code examples.\n",
    "\n",
    "Table of Contents\n",
    "\n",
    "1.Introduction to NLP\n",
    "2.Text Preprocessing\n",
    "3.Feature Extraction\n",
    "4.Text Classification\n",
    "5.Named Entity Recognition (NER)\n",
    "6.Sentiment Analysis\n",
    "7.Topic Modeling\n",
    "8.Word Embeddings\n",
    "9.Language Models\n",
    "10.Machine Translation\n",
    "11.Advanced Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to Natural Language Processing (NLP)\n",
    "Natural Language Processing (NLP) is a subfield of artificial intelligence that focuses on the interaction between computers and humans using natural language. The ultimate objective of NLP is to read, decipher, understand, and make sense of human languages in a valuable way.\n",
    "\n",
    "Key Concepts\n",
    "\n",
    "1.Tokenization: The process of breaking down text into individual words or subwords.\n",
    "2.Part-of-Speech (POS) Tagging: Assigning grammatical categories (e.g., noun, verb, adjective) to each word in a text.\n",
    "3.Named Entity Recognition (NER): Identifying and classifying named entities (e.g., person names, organizations, locations) in text.\n",
    "4.Syntax and Parsing: Analyzing the grammatical structure of sentences.\n",
    "5.Semantics: Understanding the meaning of words, phrases, and sentences.\n",
    "6.Pragmatics: Interpreting language in context.\n",
    "\n",
    "Applications of NLP\n",
    "\n",
    ".Machine Translation\n",
    ".Sentiment Analysis\n",
    ".Text Summarization\n",
    ".Question Answering Systems\n",
    ".Chatbots and Virtual Assistants\n",
    ".Information Retrieval\n",
    ".Text Classification\n",
    "\n",
    "Basic NLP Pipeline\n",
    "A typical NLP pipeline consists of the following steps:\n",
    "\n",
    "1.Text acquisition\n",
    "2.Text cleaning and preprocessing\n",
    "3.Tokenization\n",
    "4.Feature extraction\n",
    "5.Model training and evaluation\n",
    "6.Prediction or inference\n",
    "\n",
    "In the following sections, we'll explore each of these steps in detail and implement them using popular NLP libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting Started with NLTK\n",
    "NLTK (Natural Language Toolkit) is a leading platform for building Python programs to work with human language data. Let's start with a simple example using NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\didgostar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\didgostar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Natural', 'language', 'processing', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', '.']\n",
      "POS Tags: [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('subfield', 'NN'), ('of', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "text = \"Natural language processing is a subfield of artificial intelligence.\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Part-of-Speech Tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(\"POS Tags:\", pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates basic tokenization and part-of-speech tagging using NLTK. In the upcoming sections, we'll dive deeper into these concepts and explore more advanced NLP techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Preprocessing in NLP\n",
    "\n",
    "Text preprocessing is a crucial step in NLP that involves cleaning and transforming raw text data into a format suitable for analysis. This process helps to reduce noise in the text and improve the performance of NLP models.\n",
    "Common Preprocessing Steps\n",
    "\n",
    "1.Lowercasing: Converting all text to lowercase to ensure consistency.\n",
    "2.Removing punctuation: Eliminating punctuation marks that may not contribute to the meaning.\n",
    "3.Removing numbers: Removing numerical digits if they're not relevant to the analysis.\n",
    "4.Removing whitespace: Stripping extra spaces, tabs, and newlines.\n",
    "5.Removing stop words: Eliminating common words that don't carry much meaning (e.g., \"the\", \"is\", \"at\").\n",
    "6.Stemming: Reducing words to their root form (e.g., \"running\" to \"run\").\n",
    "7.Lemmatization: Similar to stemming, but ensures the root word is a valid word (e.g., \"better\" to \"good\").\n",
    "8.Handling contractions: Expanding contractions to their full form (e.g., \"don't\" to \"do not\").\n",
    "9.Removing HTML tags: Cleaning text scraped from websites.\n",
    "10.Handling emojis and special characters: Deciding whether to remove, replace, or keep these elements.\n",
    "\n",
    "Preprocessing with NLTK and spaCy\n",
    "We'll demonstrate text preprocessing using both NLTK and spaCy, two popular NLP libraries in Python.\n",
    "NLTK Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\didgostar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\didgostar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\didgostar\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quick brown fox jump lazi dog 've day\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text_nltk(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove punctuation and numbers\n",
    "    tokens = [token for token in tokens if token not in string.punctuation and not token.isdigit()]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Example usage\n",
    "text = \"The quick brown foxes are jumping over the lazy dogs! They've been doing this for 123 days.\"\n",
    "preprocessed_text = preprocess_text_nltk(text)\n",
    "print(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      3\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_text_spacy\u001b[39m(text):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess_text_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Tokenize and lemmatize\n",
    "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct and not token.is_digit]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Example usage\n",
    "text = \"The quick brown foxes are jumping over the lazy dogs! They've been doing this for 123 days.\"\n",
    "preprocessed_text = preprocess_text_spacy(text)\n",
    "print(preprocessed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next sections, we'll explore how to use these preprocessed texts for various NLP tasks such as feature extraction and text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction in NLP\n",
    "Feature extraction is the process of transforming raw text data into numerical features that can be used by machine learning algorithms. This step is crucial in NLP as it bridges the gap between human-readable text and machine-understandable input.\n",
    "Common Feature Extraction Techniques\n",
    "\n",
    "1.Bag of Words (BoW): Represents text as a multiset of words, disregarding grammar and word order.\n",
    "2.Term Frequency-Inverse Document Frequency (TF-IDF): Reflects the importance of a word in a document within a collection.\n",
    "3.Word Embeddings: Dense vector representations of words that capture semantic meanings.\n",
    "4.N-grams: Contiguous sequences of n items from a given text.\n",
    "5.Part-of-Speech (POS) Features: Grammatical features based on the role of words in sentences.\n",
    "6.Named Entity Recognition (NER) Features: Features based on identified named entities in the text.\n",
    "7.Syntactic Features: Based on the syntactic structure of sentences (e.g., dependency parsing).\n",
    "\n",
    "Implementing Feature Extraction\n",
    "We'll demonstrate how to implement Bag of Words, TF-IDF, and Word Embeddings using popular Python libraries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words (BoW) with scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample texts\n",
    "corpus = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The lazy dog sleeps all day.\",\n",
    "    \"The quick brown fox is quick.\"\n",
    "]\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the corpus and transform the texts\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the BoW representation\n",
    "print(\"Bag of Words representation:\")\n",
    "print(X.toarray())\n",
    "print(\"Feature names:\", feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF with scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample texts (same as before)\n",
    "corpus = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The lazy dog sleeps all day.\",\n",
    "    \"The quick brown fox is quick.\"\n",
    "]\n",
    "\n",
    "# Create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the corpus and transform the texts\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the TF-IDF representation\n",
    "print(\"TF-IDF representation:\")\n",
    "print(X.toarray())\n",
    "print(\"Feature names:\", feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embeddings with Gensim (Word2Vec):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample texts (same as before)\n",
    "corpus = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The lazy dog sleeps all day.\",\n",
    "    \"The quick brown fox is quick.\"\n",
    "]\n",
    "\n",
    "# Tokenize the texts\n",
    "tokenized_corpus = [word_tokenize(text.lower()) for text in corpus]\n",
    "\n",
    "# Train a Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Get the vector for a specific word\n",
    "print(\"Vector for 'fox':\", model.wv['fox'])\n",
    "\n",
    "# Find similar words\n",
    "print(\"Words similar to 'quick':\", model.wv.most_similar('quick'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These examples demonstrate how to extract features from text data using different techniques. In the next sections, we'll explore how to use these features for various NLP tasks such as text classification and clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
