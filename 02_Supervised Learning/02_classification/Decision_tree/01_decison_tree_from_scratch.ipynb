{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "In the following code, we have loaded a dataset containing information for two weeks, indicating whether a game was played or not. The dataset includes four different features (conditions): outlook, temperature, humidity, and wind. The last column, which is a boolean, shows whether a game was played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outlook</th>\n",
       "      <th>temp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windy</th>\n",
       "      <th>play</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sunny</td>\n",
       "      <td>hot</td>\n",
       "      <td>high</td>\n",
       "      <td>False</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sunny</td>\n",
       "      <td>hot</td>\n",
       "      <td>high</td>\n",
       "      <td>True</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>overcast</td>\n",
       "      <td>hot</td>\n",
       "      <td>high</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rainy</td>\n",
       "      <td>mild</td>\n",
       "      <td>high</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rainy</td>\n",
       "      <td>cool</td>\n",
       "      <td>normal</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rainy</td>\n",
       "      <td>cool</td>\n",
       "      <td>normal</td>\n",
       "      <td>True</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>overcast</td>\n",
       "      <td>cool</td>\n",
       "      <td>normal</td>\n",
       "      <td>True</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sunny</td>\n",
       "      <td>mild</td>\n",
       "      <td>high</td>\n",
       "      <td>False</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sunny</td>\n",
       "      <td>cool</td>\n",
       "      <td>normal</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rainy</td>\n",
       "      <td>mild</td>\n",
       "      <td>normal</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sunny</td>\n",
       "      <td>mild</td>\n",
       "      <td>normal</td>\n",
       "      <td>True</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>overcast</td>\n",
       "      <td>mild</td>\n",
       "      <td>high</td>\n",
       "      <td>True</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>overcast</td>\n",
       "      <td>hot</td>\n",
       "      <td>normal</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rainy</td>\n",
       "      <td>mild</td>\n",
       "      <td>high</td>\n",
       "      <td>True</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     outlook  temp humidity  windy play\n",
       "1      sunny   hot     high  False   no\n",
       "2      sunny   hot     high   True   no\n",
       "3   overcast   hot     high  False  yes\n",
       "4      rainy  mild     high  False  yes\n",
       "5      rainy  cool   normal  False  yes\n",
       "6      rainy  cool   normal   True   no\n",
       "7   overcast  cool   normal   True  yes\n",
       "8      sunny  mild     high  False   no\n",
       "9      sunny  cool   normal  False  yes\n",
       "10     rainy  mild   normal  False  yes\n",
       "11     sunny  mild   normal   True  yes\n",
       "12  overcast  mild     high   True  yes\n",
       "13  overcast   hot   normal  False  yes\n",
       "14     rainy  mild     high   True   no"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#numpy and pandas initialization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#Loading the PlayTennis data\n",
    "PlayTennis = pd.read_csv(\"./files/PlayTennis.csv\")\n",
    "PlayTennis.index= list(range(1,PlayTennis.shape[0]+1))\n",
    "PlayTennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For such a dataset, a decision tree could look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./files/decision-tree-tennis2.svg' style=\"background-color: white\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DT learning algorithms\n",
    "\n",
    "Decision tree learning algorithms are methods used to create decision trees, which are models for making decisions based on data\n",
    "\n",
    "- Hunt\n",
    "- ID3 (Iterative Dichotomister 3)\n",
    "- C4.5\n",
    "- CART (Classification And Regression Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hunt's Algorithm\n",
    "Hunt's algorithm is one of the earliest methods for constructing decision trees. It forms the basis for many other decision tree algorithms. The process involves recursively partitioning the data set into subsets based on the most significant attribute until each subset contains instances of a single class or meets a stopping criterion.\n",
    "\n",
    "### ID3 (Iterative Dichotomiser 3)\n",
    "ID3, developed by Ross Quinlan, is a popular algorithm for creating decision trees. It uses a top-down, greedy approach to select the attribute that maximizes information gain (a measure based on entropy) at each node. The process continues recursively until all data is classified or no further information gain can be achieved.\n",
    "\n",
    "### C4.5\n",
    "C4.5 is an extension of the ID3 algorithm, also developed by Ross Quinlan. It improves upon ID3 by handling both continuous and discrete attributes, dealing with missing values, and pruning trees to avoid overfitting. C4.5 uses gain ratio instead of information gain to select the best attribute, which helps to mitigate the bias towards attributes with many values.\n",
    "\n",
    "### CART (Classification and Regression Trees)\n",
    "CART, developed by Breiman et al., is another widely used decision tree algorithm. Unlike ID3 and C4.5, which are primarily used for classification, CART can handle both classification and regression tasks. It uses the Gini impurity or mean squared error (for regression) to split the data at each node. CART also includes a pruning step to simplify the tree and improve its generalization ability.\n",
    "\n",
    "These algorithms are fundamental in machine learning and data mining, providing a clear and interpretable way to model decision-making processes. Do you have a specific application in mind for these algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "\n",
    "To determine which feature should be in the root node, decision tree uses `Entropy`.\n",
    "\n",
    "### Entropy \n",
    "\n",
    "Entropy in a decision tree is a measure of the impurity or disorder within a set of data. It helps determine how a decision tree splits the data at each node to create branches that lead to the most homogeneous subsets possible.\n",
    "\n",
    "$$\n",
    "E(S) = - \\sum_{i=1}^{c} p_i \\log_2(p_i)\n",
    "$$\n",
    "where \\( S \\) is the dataset, \\( c \\) is the number of classes, and \\( p_i \\) is the proportion of instances belonging to class \\( i \\).\n",
    "\n",
    "example:\n",
    "\n",
    "For class of red, $p_i = 3/4$ and for class of blue $p_i = 3/4$. therefore entropy is calculate as follow:\n",
    "\n",
    "$$\n",
    "E(S) = - (3/4 * \\log_2(3/4) + 1/4 * \\log_2(1/4)) = 0.81\n",
    "$$\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src='./files/entropy2.svg' style='background-color: white' />\n",
    "</div>\n",
    "\n",
    "For class of red, $p_i = 0/4$ and for class of blue $p_i = 4/4$. therefore entropy is calculate as follow:\n",
    "\n",
    "$$\n",
    "E(S) = - (0/4 * \\log_2(0/4) + 1/4 * \\log_2(4/4)) = 0\n",
    "$$\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src='./files/entropy3.svg' style='background-color: white' />\n",
    "</div>\n",
    "\n",
    "\n",
    "For class of red, $p_i = 3/4$ and for class of blue $p_i = 3/4$. therefore entropy is calculate as follow:\n",
    "\n",
    "$$\n",
    "E(S) = - (2/4 * \\log_2(2/4) + 2/4 * \\log_2(2/4)) = 1\n",
    "$$\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src='./files/entropy4.svg' style='background-color: white' />\n",
    "</div>\n",
    "\n",
    "\n",
    "**The lower the entropy, the purer our data is.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following figure we can see that entropy is a number between zero and one, and it is 1 when number of red data is equal to number of blue data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "number_of_data = 50\n",
    "E = []\n",
    "for i in range(number_of_data+1):\n",
    "    number_of_red_data = i\n",
    "    number_of_blue_data = number_of_data - i\n",
    "    \n",
    "    pi_red = (number_of_red_data/number_of_data)\n",
    "    pi_blue = (number_of_blue_data/number_of_data)\n",
    "    \n",
    "    E.append(-1*(pi_red*np.log2(pi_red, where=pi_red > 0) + pi_blue*np.log2(pi_blue, where=pi_blue > 0)))\n",
    "    \n",
    "plt.scatter(list(range(number_of_data+1)), E)\n",
    "plt.title(\"Entropy respect to number of data\")\n",
    "plt.xlabel(\"Number of red data\")\n",
    "plt.ylabel(\"Entropy\")\n",
    "plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8112781244591328\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = -1*(3/4*np.log2(3/4) + 1/4*np.log2(1/4))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Gain:\n",
    "$$\n",
    "\\text{IG} = E(\\text{parent}) - [\\text{weighted average}] * E(\\text{children})\n",
    "$$\n",
    "- where $E$ is called Entropy\n",
    "\n",
    "In decision trees, information gain is a measure used to determine which feature to split on at each step in the tree-building process. It helps in selecting the attribute that best separates the data into different classes.\n",
    "\n",
    "Information gain is the reduction in entropy after a dataset is split on an attribute. It is calculated as:\n",
    "\n",
    "In decision trees, **information gain** is a measure used to determine which feature to split on at each step in the tree-building process. It helps in selecting the attribute that best separates the data into different classes.\n",
    "\n",
    "### How Information Gain Works\n",
    "\n",
    "**Information Gain Calculation**:\n",
    "   - Information gain is the reduction in entropy after a dataset is split on an attribute. It is calculated as:\n",
    "     $$\n",
    "     \\text{Gain}(S, A) = \\text{Entropy}(S) - \\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} \\text{Entropy}(S_v)\n",
    "     $$\n",
    "     where:\n",
    "     - $ S $ is the original dataset.\n",
    "     - $ A $ is the attribute being considered for the split.\n",
    "     - $ S_v $ is the subset of \\( S \\) for which attribute \\( A \\) has value \\( v \\).\n",
    "     - $\\text{Values}(A)$ are the possible values of attribute \\( A \\).\n",
    "     - $ |S| $ and $ |S_v| $ are the sizes of the datasets \\( S \\) and \\( S_v \\), respectively.\n",
    "\n",
    "\n",
    "Information gain helps in building more efficient and accurate decision trees by ensuring that each split maximally reduces uncertainty, leading to better classification performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "In the following dataset, we want to calculate the information gain of Outlook feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outlook</th>\n",
       "      <th>temp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windy</th>\n",
       "      <th>play</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sunny</td>\n",
       "      <td>hot</td>\n",
       "      <td>high</td>\n",
       "      <td>False</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sunny</td>\n",
       "      <td>hot</td>\n",
       "      <td>high</td>\n",
       "      <td>True</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>overcast</td>\n",
       "      <td>hot</td>\n",
       "      <td>high</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rainy</td>\n",
       "      <td>mild</td>\n",
       "      <td>high</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rainy</td>\n",
       "      <td>cool</td>\n",
       "      <td>normal</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rainy</td>\n",
       "      <td>cool</td>\n",
       "      <td>normal</td>\n",
       "      <td>True</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>overcast</td>\n",
       "      <td>cool</td>\n",
       "      <td>normal</td>\n",
       "      <td>True</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sunny</td>\n",
       "      <td>mild</td>\n",
       "      <td>high</td>\n",
       "      <td>False</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sunny</td>\n",
       "      <td>cool</td>\n",
       "      <td>normal</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rainy</td>\n",
       "      <td>mild</td>\n",
       "      <td>normal</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sunny</td>\n",
       "      <td>mild</td>\n",
       "      <td>normal</td>\n",
       "      <td>True</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>overcast</td>\n",
       "      <td>mild</td>\n",
       "      <td>high</td>\n",
       "      <td>True</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>overcast</td>\n",
       "      <td>hot</td>\n",
       "      <td>normal</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rainy</td>\n",
       "      <td>mild</td>\n",
       "      <td>high</td>\n",
       "      <td>True</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     outlook  temp humidity  windy play\n",
       "1      sunny   hot     high  False   no\n",
       "2      sunny   hot     high   True   no\n",
       "3   overcast   hot     high  False  yes\n",
       "4      rainy  mild     high  False  yes\n",
       "5      rainy  cool   normal  False  yes\n",
       "6      rainy  cool   normal   True   no\n",
       "7   overcast  cool   normal   True  yes\n",
       "8      sunny  mild     high  False   no\n",
       "9      sunny  cool   normal  False  yes\n",
       "10     rainy  mild   normal  False  yes\n",
       "11     sunny  mild   normal   True  yes\n",
       "12  overcast  mild     high   True  yes\n",
       "13  overcast   hot   normal  False  yes\n",
       "14     rainy  mild     high   True   no"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PlayTennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information gain for outlook feature is calculated as follow:\n",
    "\n",
    "$$\n",
    "G(S,outlook) = E(s) - ( \\frac{|S_{rainy}|}{|S|}*E(S_{rainy}) +  \\frac{|S_{sunny}|}{|S|}*E(S_{sunny}) + \\frac{|S_{overcast}|}{|S|}*E(S_{overcast}) )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, we have 5 instances belonging to No class and 9 instances belonging to Yes. Therefore, the general entropy is calculated as follow:\n",
    "\n",
    "$$\n",
    "E(S) = - (9/14 * \\log_2(9/14) + 5/14 * \\log_2(5/14)) = 0.94\n",
    "$$\n",
    "\n",
    "For outlook feature, we have three classes: sunny, rainy, overcast.\n",
    "We have 5 instances When outlook is rainy. Therefore:\n",
    "\n",
    "$$\n",
    "\\frac{|S_{rainy}|}{|S|} = \\frac{5}{14} = 0.36\n",
    "$$\n",
    "\n",
    "3 of them belong to Yes and the other 2 belong to No class. Therefore\n",
    "\n",
    "$$\n",
    "E(S_{rainy}) = - (3/5 * \\log_2(3/5) + 2/5 * \\log_2(2/5)) = 0.97\n",
    "$$\n",
    "\n",
    "With the same approach:\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{|S_{sunny}|}{|S|} = \\frac{5}{14} = 0.36\n",
    "$$\n",
    "$$\n",
    "E(S_{sunny}) = - (2/5 * \\log_2(2/5) + 3/5 * \\log_2(3/5)) = 0.97\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{|S_{overcast}|}{|S|} = \\frac{4}{14} = 0.29\n",
    "$$\n",
    "$$\n",
    "E(S_{overcast}) = - (4/4 * \\log_2(4/4) + 0/4 * \\log_2(0/4)) = 0\n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\n",
    "G(S,outlook) = 0.94 - ( 0.36 * 0.97 +  0.36 * 0.97 - 0.29 * 0 ) = 0.24 \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing all the calculations for all of the featues, we select the feature with the highest information gain as the root node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9709505944546686\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = -1*(3/5*np.log2(3/5) + 2/5*np.log2(2/5))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Neighbourhood | # of rooms | Affordable (boolean) |\n",
    "|----------|----------|----------|\n",
    "| West   | 3 | <span style=\"color: green;\">Yes</span> |\n",
    "| West   | 5 | <span style=\"color: green;\">Yes</span> |\n",
    "| West   | 2 | <span style=\"color: green;\">Yes</span> |\n",
    "| East   | 3 | <span style=\"color: green;\">Yes</span> |\n",
    "| East   | 4 | <span style=\"color: green;\">Yes</span> |\n",
    "| East   | 6 | <span style=\"color: red;\">No</span> |\n",
    "| East   | 5 | <span style=\"color: red;\">No</span> |\n",
    "| East   | 2 | <span style=\"color: green;\">Yes</span> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = './files/decision_tree.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What needs to be decided on?\n",
    "\n",
    "- Split feature\n",
    "- Split Point\n",
    "- When to stop splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps\n",
    "\n",
    "\n",
    "- Training:\n",
    "    - Given the whole dataset:\n",
    "        - Calculate information gain with each possible split\n",
    "        - Divide set with that feature and value that gives the most information gain (IG)\n",
    "        - Divide tree and do the same for all created branches...\n",
    "        - ... until a stopping criteria is reached\n",
    "\n",
    "- Testing: \n",
    "    - Given a data point:\n",
    "        - Follow the tree until you reach a leaf node\n",
    "        - Return the most common class label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory and Terms\n",
    "\n",
    "\n",
    "### Entropy \n",
    "\n",
    "Entropy in a decision tree is a measure of the impurity or disorder within a set of data. It helps determine how a decision tree splits the data at each node to create branches that lead to the most homogeneous subsets possible.\n",
    "\n",
    "### Key Points about Entropy:\n",
    "1. **Definition**: Entropy quantifies the amount of uncertainty or impurity in a dataset. In the context of decision trees, it measures how mixed the classes are within a node.\n",
    "2. **Calculation**: The entropy \\( H \\) of a dataset is calculated using the formula:\n",
    "   $$\n",
    "   E(S) = - \\sum_{i=1}^{c} p_i \\log_2(p_i)\n",
    "   $$\n",
    "   where \\( S \\) is the dataset, \\( c \\) is the number of classes, and \\( p_i \\) is the proportion of instances belonging to class \\( i \\).\n",
    "\n",
    "3. **Purpose**: By calculating entropy, a decision tree algorithm can decide the best attribute to split the data. The goal is to choose splits that minimize entropy, leading to more pure (homogeneous) subsets¹².\n",
    "4. **Information Gain**: When a dataset is split based on an attribute, the change in entropy is called information gain. The attribute with the highest information gain is chosen for the split, as it provides the most significant reduction in uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping criteria\n",
    "\n",
    "    - maximum depth\n",
    "    - minimum number of samples\n",
    "    - min impurity decrease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None,*,value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "        \n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=100, n_features=None):\n",
    "        self.min_samples_split=min_samples_split\n",
    "        self.max_depth=max_depth\n",
    "        self.n_features=n_features\n",
    "        self.root=None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1],self.n_features)\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_feats = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        # check the stopping criteria\n",
    "        if (depth>=self.max_depth or n_labels==1 or n_samples<self.min_samples_split):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        feat_idxs = np.random.choice(n_feats, self.n_features, replace=False)\n",
    "\n",
    "        # find the best split\n",
    "        best_feature, best_thresh = self._best_split(X, y, feat_idxs)\n",
    "\n",
    "        # create child nodes\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)\n",
    "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n",
    "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n",
    "        return Node(best_feature, best_thresh, left, right)\n",
    "\n",
    "\n",
    "    def _best_split(self, X, y, feat_idxs):\n",
    "        best_gain = -1\n",
    "        split_idx, split_threshold = None, None\n",
    "\n",
    "        for feat_idx in feat_idxs:\n",
    "            X_column = X[:, feat_idx]\n",
    "            thresholds = np.unique(X_column)\n",
    "\n",
    "            for thr in thresholds:\n",
    "                # calculate the information gain\n",
    "                gain = self._information_gain(y, X_column, thr)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_threshold = thr\n",
    "\n",
    "        return split_idx, split_threshold\n",
    "\n",
    "\n",
    "    def _information_gain(self, y, X_column, threshold):\n",
    "        # parent entropy\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        # create children\n",
    "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
    "\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return 0\n",
    "        \n",
    "        # calculate the weighted avg. entropy of children\n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
    "        child_entropy = (n_l/n) * e_l + (n_r/n) * e_r\n",
    "\n",
    "        # calculate the IG\n",
    "        information_gain = parent_entropy - child_entropy\n",
    "        return information_gain\n",
    "\n",
    "    def _split(self, X_column, split_thresh):\n",
    "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
    "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        hist = np.bincount(y)\n",
    "        ps = hist / len(y)\n",
    "        return -np.sum([p * np.log(p) for p in ps if p>0])\n",
    "\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        value = counter.most_common(1)[0][0]\n",
    "        return value\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "In this example we use the Breast Cancer Wisconsin dataset, which is commonly used for binary classification tasks. Here are some key details:\n",
    "\n",
    "- **Dataset Characteristics**:\n",
    "  - **Classes**: 2 (malignant and benign)\n",
    "  - **Samples per class**: 212 (malignant), 357 (benign)\n",
    "  - **Total samples**: 569\n",
    "  - **Dimensionality**: 30 features\n",
    "  - **Feature types**: Real, positive values\n",
    "  \n",
    "- **Return Values**:\n",
    "  - **data**: The data matrix (569 samples, 30 features).\n",
    "  - **target**: The classification target (569 samples).\n",
    "  - **feature_names**: Names of the dataset columns.\n",
    "  - **target_names**: Names of the target classes.\n",
    "  - **DESCR**: Full description of the dataset.\n",
    "  - **filename**: Path to the location of the data file.\n",
    "\n",
    "\n",
    "This dataset is great for practicing classification algorithms and understanding the basics of machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "Target: ['malignant' 'benign']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  target  \n",
       "0                  0.2654          0.4601                  0.11890       0  \n",
       "1                  0.1860          0.2750                  0.08902       0  \n",
       "2                  0.2430          0.3613                  0.08758       0  \n",
       "3                  0.2575          0.6638                  0.17300       0  \n",
       "4                  0.1625          0.2364                  0.07678       0  \n",
       "..                    ...             ...                      ...     ...  \n",
       "564                0.2216          0.2060                  0.07115       0  \n",
       "565                0.1628          0.2572                  0.06637       0  \n",
       "566                0.1418          0.2218                  0.07820       0  \n",
       "567                0.2650          0.4087                  0.12400       0  \n",
       "568                0.0000          0.2871                  0.07039       1  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "\n",
    "bc = datasets.load_breast_cancer()\n",
    "# columns = bc.columns\n",
    "X, y = bc.data, bc.target\n",
    "# df = pd.DataFrame(X)\n",
    "\n",
    "# Get the column names\n",
    "column_names = bc.feature_names\n",
    "label_names = bc.target_names\n",
    "print(f'Features: {column_names}')\n",
    "print(f'Target: {label_names}')\n",
    "# Convert column names to DataFrame\n",
    "df = pd.DataFrame(X, columns=column_names)\n",
    "df['target'] = y\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1234\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = DecisionTree(max_depth=10)\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "def accuracy(y_test, y_pred):\n",
    "    return np.sum(y_test == y_pred) / len(y_test)\n",
    "\n",
    "acc = accuracy(y_test, predictions)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:\n",
    "\n",
    "- https://www.youtube.com/watch?v=NxEHSAfFlK8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise:\n",
    "\n",
    "Train a decision tree for Iris data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
