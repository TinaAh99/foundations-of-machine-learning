{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Theory and Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-Nearest Neighbors (KNN) algorithm is a simple, yet powerful, supervised machine learning algorithm used for both classification and regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it Works:\n",
    "1. **Training Phase**: KNN doesn't have an explicit training phase. Instead, it stores the entire training dataset.\n",
    "2. **Prediction Phase**:\n",
    "   - For a given new data point, KNN identifies the 'K' closest data points from the training set. The closeness is typically measured using distance metrics like Euclidean distance.\n",
    "   - **Classification**: The new data point is assigned the class that is most common among its K nearest neighbors (majority vote).\n",
    "   - **Regression**: The value of the new data point is predicted by averaging the values of its K nearest neighbors.\n",
    "\n",
    "### Key Characteristics:\n",
    "- **Non-parametric**: KNN makes no assumptions about the underlying data distribution.\n",
    "- **Lazy Learning**: It doesn't learn a model but makes decisions based on the entire dataset at prediction time.\n",
    "\n",
    "### Example:\n",
    "Imagine you have a dataset of fruits with features like weight and color. If you want to classify a new fruit, KNN will look at the K nearest fruits in the dataset and assign the new fruit to the most common category among those neighbors.\n",
    "\n",
    "KNN is appreciated for its simplicity and effectiveness, especially in scenarios where the decision boundary is irregular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
